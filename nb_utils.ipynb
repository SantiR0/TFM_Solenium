{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c8619f-c587-4ce0-bea6-f0ba58cdfaeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import logging\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import from_utc_timestamp, date_format\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import IntegerType, FloatType, StringType, BooleanType, TimestampType, DataType,  StructType, StructField\n",
    "\n",
    "class MedallionConfig:\n",
    "    \"\"\"\n",
    "    A class to configure and manage environment, ingestion, and connector settings for the medallion architecture (Bronze, Silver, Gold layers).\n",
    "    \n",
    "    Attributes:\n",
    "        ingest_name (str): The name of the ingestion process.\n",
    "        spark (SparkSession): Spark session used for querying and data manipulation.\n",
    "    \"\"\"\n",
    "\n",
    "    env_conf_path        = '/dbfs/FileStore/parameters/environment/environment_conf.json'\n",
    "    ingestion_conf_path  = '/dbfs/FileStore/parameters/ingestion'\n",
    "    connectors_conf_path = '/dbfs/FileStore/parameters/connectors'\n",
    "\n",
    "    def __init__(self, ingest_name):\n",
    "        \"\"\"\n",
    "        Initializes MedallionConfig with ingestion name and loads environment, storage, and connector configurations.\n",
    "\n",
    "        Parameters:\n",
    "        ingest_name: str\n",
    "            The name of the ingestion process.\n",
    "        \"\"\"\n",
    "        self.ingest_name = ingest_name\n",
    "        self.spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "        self._load_env_conf()\n",
    "        self._connect_to_storage()\n",
    "\n",
    "        self._ingestion_conf()\n",
    "        self._connector_conf()\n",
    "        self._set_medallion_path()\n",
    "    \n",
    "    def _load_env_conf(self):\n",
    "        \"\"\"\n",
    "        Loads the environment configuration from a JSON file (located in dbfs) and sets environment variables.\n",
    "\n",
    "        Raises:\n",
    "        Exception\n",
    "            If there is an issue loading the environment configuration file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.env_conf_path, \"r\") as file:\n",
    "                env_file = json.load(file)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading environment medallion configuration file: {str(e)}\")\n",
    "\n",
    "        self._storage_account_name       = env_file.get('storage_account_name')\n",
    "        self.__storage_account_access_key = env_file.get('storage_account_access_key')\n",
    "        self.silver_format              = env_file.get('silver_format')\n",
    "        self.gold_format                = env_file.get('gold_format')\n",
    "        \n",
    "\n",
    "    def _connect_to_storage(self):\n",
    "        \"\"\"\n",
    "        Configures the connection to the Azure storage account using the account name and access key.\n",
    "        \"\"\"\n",
    "        self.spark.conf.set(f\"fs.azure.account.key.{self._storage_account_name}.dfs.core.windows.net\",self.__storage_account_access_key)\n",
    "    \n",
    "    def _set_medallion_path(self):\n",
    "        \"\"\"\n",
    "        Sets the paths for the Bronze, Silver, and Gold layers based on the storage account name and table.\n",
    "        \"\"\"\n",
    "        self.bronze_path  = f\"abfss://bronze@{self._storage_account_name}.dfs.core.windows.net\"\n",
    "        self.silver_path  = f\"abfss://silver@{self._storage_account_name}.dfs.core.windows.net\"\n",
    "        self.gold_path    = f\"abfss://gold@{self._storage_account_name}.dfs.core.windows.net\"\n",
    "\n",
    "        self.bronze_path_file = os.path.join(self.bronze_path, self.table)\n",
    "        self.silver_path_file = os.path.join(self.silver_path, self.table)\n",
    "        self.checkpoint_silver_path  = os.path.join(self.silver_path, 'checkpoint')\n",
    "\n",
    "    def _ingestion_conf(self):\n",
    "        \"\"\"\n",
    "        Loads the ingestion configuration from a JSON file (located in dbfs) and extracts source and sink configurations.\n",
    "\n",
    "        Raises:\n",
    "        Exception\n",
    "            If there is an issue loading the ingestion configuration file.\n",
    "        \"\"\"\n",
    "        self.ingestion_conf_path_file = os.path.join(self.ingestion_conf_path, self.ingest_name + '.json')\n",
    "        try:\n",
    "            with open(self.ingestion_conf_path_file, \"r\") as file:\n",
    "                ingest_config = json.load(file)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading ingestion configuration file: {str(e)}\")\n",
    "\n",
    "\n",
    "        self.source   = ingest_config.get(\"source\")\n",
    "        self.sink     = ingest_config.get(\"sink\")\n",
    "\n",
    "        self.db       = self.source.get(\"db\")\n",
    "        self.table    = self.source.get(\"table\")\n",
    "        self.columns  = self.source.get(\"columns\")\n",
    "        self.delta    = self.source.get(\"delta_column\")\n",
    "        self.calendar = self.source.get(\"calendar\", {})\n",
    "\n",
    "        source_sink = self.sink.get(\"source\")\n",
    "        self.bronze_format = source_sink.get(\"format\")\n",
    "        self.bronze_opts   = source_sink.get(\"options\")\n",
    "\n",
    "    def _connector_conf(self):\n",
    "        \"\"\"\n",
    "        Loads the connector configuration from a JSON file (located in dbfs) for setting up connections to external data sources.\n",
    "\n",
    "        Raises:\n",
    "        Exception\n",
    "            If there is an issue loading the connector configuration file.\n",
    "        \"\"\"\n",
    "        connector_path = self.source.get(\"connector\")\n",
    "        connector_conf_path_file = os.path.join(self.connectors_conf_path, connector_path)\n",
    "        try: \n",
    "            with open(connector_conf_path_file, \"r\") as file:\n",
    "                connector_file = json.load(file)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading connector configuration file: {str(e)}\")\n",
    "\n",
    "        self.db_format    = connector_file.get(\"format\")\n",
    "        self.db_opts      = connector_file.get(\"options\")\n",
    "    \n",
    "    def create_schema_table(self, path: str, table: str):\n",
    "        \"\"\"\n",
    "        Creates a schema and a Delta table if they do not exist.\n",
    "\n",
    "        Parameters:\n",
    "        path: str\n",
    "            The path where the table data is stored.\n",
    "        table: str\n",
    "            The name of the table to be created.\n",
    "        \"\"\"\n",
    "        create_schema_sql = f\"\"\" CREATE SCHEMA IF NOT EXISTS {self.db};\"\"\"\n",
    "        use_schema_sql = f\"\"\"USE {self.db}; \"\"\"\n",
    "        create_table_sql = f\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS {self.db}.{table} \n",
    "                    USING DELTA \n",
    "                    LOCATION '{path}'\n",
    "        \"\"\"\n",
    "        self.spark.sql(create_schema_sql)\n",
    "        self.spark.sql(use_schema_sql)\n",
    "        self.spark.sql(create_table_sql)\n",
    "\n",
    "    def set_logger(self):\n",
    "        \"\"\"\n",
    "        Sets up logging configuration for the ingest application.\n",
    "\n",
    "        This function configures the root logger by removing any existing handlers and setting up a new logger configuration.\n",
    "        It defines a logging format that includes the timestamp, logger name, log level, and log message. A specific logger\n",
    "        for the application ('engine_ingest') is created and set to the INFO level. And the logging level for the 'py4j' logger is set to WARNING.\n",
    "        \"\"\"\n",
    "        for handler in logging.root.handlers[:]:\n",
    "            logging.root.removeHandler(handler)\n",
    "\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format= '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers= [logging.StreamHandler()]\n",
    "        )\n",
    "\n",
    "        app_logger = logging.getLogger('engine_ingest')\n",
    "        app_logger.setLevel(logging.INFO)\n",
    "\n",
    "        logging.getLogger('py4j').setLevel(logging.WARNING)\n",
    "\n",
    "    \n",
    "class DbIngestionToBronze:\n",
    "    \"\"\"\n",
    "    A class to handle the ingestion of data from the database to the Bronze layer in a medallion architecture.\n",
    "    \n",
    "    Attributes:\n",
    "        medallion_config (MedallionConfig): Configuration object containing paths and settings for the medallion architecture.\n",
    "        start_date (str): Optional start date for filtering data.\n",
    "        end_date (str): Optional end date for filtering data.\n",
    "        flag_range (str): Whether to use a date range.\n",
    "        spark (SparkSession): Spark session used for querying and data manipulation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, medallion_config: MedallionConfig, start_date = None, end_date = None, flag_range = None):\n",
    "        \"\"\"\n",
    "        Initializes the ingestion process with configuration and optional date range.\n",
    "\n",
    "        Parameters:\n",
    "        medallion_config: MedallionConfig\n",
    "            Configuration object containing paths and settings for the medallion architecture.\n",
    "        start_date: str, optional\n",
    "            The start date for filtering data (default is None).\n",
    "        end_date: str, optional\n",
    "            The end date for filtering data (default is None).\n",
    "        flag_range: str, optional\n",
    "            A flag to indicate whether to use a date range (default is None).\n",
    "        \"\"\"\n",
    "        self.medallion_config = medallion_config\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.flag_range = flag_range\n",
    "        self.spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    def _generate_sql_root(self):\n",
    "        \"\"\"\n",
    "        Generates the SQL root query to extract data from the source database.\n",
    "\n",
    "        Returns:\n",
    "        str\n",
    "            SQL query string for the specified table and columns.\n",
    "        \"\"\"\n",
    "        columns = ', '.join(self.medallion_config.columns.keys())\n",
    "        table  = self.medallion_config.table\n",
    "        return f\"SELECT {columns} FROM PUBLIC.{table}\"\n",
    "\n",
    "    def _calculate_date_range(self):\n",
    "        \"\"\"\n",
    "        Calculates the start and end date for the SQL query, either from provided values or default values in the configuration.\n",
    "\n",
    "        Returns:\n",
    "        tuple\n",
    "            A tuple containing start_date and end_date.\n",
    "        \"\"\"\n",
    "        if self.flag_range and self.start_date and self.end_date:\n",
    "            end_date  = pd.Timestamp(self.end_date, tz='UTC')\n",
    "            start_date = pd.Timestamp(self.start_date, tz='UTC')\n",
    "        else:\n",
    "            end_date = pd.Timestamp(self.medallion_config.calendar.get(\"end_date\", pd.Timestamp.now()), tz='UTC')\n",
    "            start_date = pd.Timestamp(self.medallion_config.calendar.get(\"start_date\", end_date - pd.Timedelta(days=3)), tz='UTC')\n",
    "        return start_date, end_date\n",
    "        \n",
    "    def generate_sql_statement(self):\n",
    "        \"\"\"\n",
    "        Generates the complete SQL query statement including filtering by date range if necessary.\n",
    "\n",
    "        Returns:\n",
    "        str\n",
    "            SQL statement string with or without date filtering.\n",
    "        \"\"\"\n",
    "        sql_root = self._generate_sql_root()\n",
    "\n",
    "        if self.medallion_config.delta:\n",
    "            start_date, end_date = self._calculate_date_range()\n",
    "            return  f\"{sql_root} WHERE {self.medallion_config.delta} BETWEEN '{start_date}' AND '{end_date}'\"\n",
    "        else:\n",
    "            return sql_root\n",
    "    \n",
    "    def _convert_time_stamp(self, df: DataFrame, columns: dict) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Converts specified columns from timestamp format to string format.\n",
    "\n",
    "        Parameters:\n",
    "        df: DataFrame\n",
    "            Spark DataFrame containing the data.\n",
    "        columns: dict\n",
    "            Dictionary of columns and their types to identify which columns to convert.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame\n",
    "            Spark DataFrame with converted timestamp columns.\n",
    "        \"\"\"\n",
    "        \n",
    "        columns_to_parse = [column for column, col_type in columns.items() if col_type == \"timestamp\"]\n",
    "\n",
    "        for column in columns_to_parse: \n",
    "                df = df.withColumn(column, F.col(column).cast('string'))\n",
    "\n",
    "        return df\n",
    "\n",
    "    def load_data(self, sql_statement: str):\n",
    "        \"\"\"\n",
    "        Loads data from the source database into a Spark DataFrame based on the SQL query.\n",
    "\n",
    "        Parameters:\n",
    "        sql_statement: str\n",
    "            The SQL query to extract data from the source database.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame\n",
    "            Spark DataFrame containing the loaded data.\n",
    "\n",
    "        Raises:\n",
    "        RuntimeError\n",
    "            If there is an error while loading the data from the source.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            df = self.spark.read.format(self.medallion_config.db_format)\\\n",
    "                    .option(\"query\", sql_statement)\\\n",
    "                    .options(**self.medallion_config.db_opts)\\\n",
    "                    .load()\n",
    "            df = self._convert_time_stamp(df, self.medallion_config.columns)\n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading data with the SQL statement: {sql_statement}, error: {str(e)}\") \n",
    "\n",
    "\n",
    "    def write_data(self, df: DataFrame):\n",
    "        \"\"\"\n",
    "        Writes the DataFrame data to the Bronze layer as Parquet files.\n",
    "\n",
    "        Parameters:\n",
    "        df: DataFrame\n",
    "            Spark DataFrame containing the data to be written.\n",
    "        Raises:\n",
    "        RuntimeError\n",
    "            If there is an error while writing the data to the Bronze layer.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df.write.mode('append').parquet(self.medallion_config.bronze_path_file)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error writing data to the Bronze layer, error: {str(e)}\")\n",
    "    \n",
    "    def update_calendar(self):\n",
    "        \"\"\"\n",
    "        Updates the start_date in the calendar configuration to the current date for the next execution and saves the updated configuration to json file.\n",
    "       \n",
    "        Raises:\n",
    "        Exception\n",
    "            If there is an issue writing the updated configuration to json file.\n",
    "        \"\"\"\n",
    "        self.medallion_config.calendar[\"start_date\"] = dt.datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "        try:\n",
    "            with open(self.medallion_config.ingestion_conf_path_file, 'w') as file:\n",
    "                json.dump({\"source\":self.medallion_config.source, \"sink\": self.medallion_config.sink}, file, indent = 4)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error writing updated calendar configuration to json file, error: {str(e)}\")\n",
    "\n",
    "class BronzeIngestionToSilver:\n",
    "    \"\"\"\n",
    "    A class to handle the ingestion of data from the Bronze layer to the Silver layer in a medallion architecture.\n",
    "\n",
    "    Attributes:\n",
    "        medallion_config (MedallionConfig): Configuration object containing paths and settings for the medallion architecture.\n",
    "        spark (SparkSession): Spark session used for querying and data manipulation.\n",
    "    \"\"\"\n",
    "    def __init__(self, medallion_config: MedallionConfig):\n",
    "        \"\"\"\n",
    "        Initializes the ingestion process with the configuration for the Silver layer.\n",
    "\n",
    "        Parameters:\n",
    "        medallion_config: MedallionConfig\n",
    "            Configuration object containing paths and settings for the medallion architecture.\n",
    "        \"\"\"\n",
    "        self.medallion_config = medallion_config\n",
    "        self.spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    def process_batch(self, df_batch: DataFrame, batch_id: int):\n",
    "        \"\"\"\n",
    "        Processes a batch of data from Bronze to Silver, merging new records with existing ones.\n",
    "\n",
    "        Parameters:\n",
    "        df_batch: DataFrame\n",
    "            The Spark DataFrame containing the batch of data to process.\n",
    "        batch_id: int\n",
    "            The ID of the batch being processed.\n",
    "        \"\"\"\n",
    "        if not DeltaTable.isDeltaTable(self.spark, self.medallion_config.silver_path_file):\n",
    "            self.spark.createDataFrame([], df_batch.schema)\\\n",
    "                .write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .partitionBy(\"date\")\\\n",
    "                .save(self.medallion_config.silver_path_file)\n",
    "\n",
    "        dates_to_insert = [row.date for row in df_batch.select(F.col(\"date\")).distinct().collect()]\n",
    "        \n",
    "        df_exists = self.spark.read.format(\"delta\")\\\n",
    "                            .load(self.medallion_config.silver_path_file)\\\n",
    "                            .filter(F.col(\"date\")\\\n",
    "                            .isin(dates_to_insert))\\\n",
    "                            .select(\"id\").cache()\n",
    "\n",
    "        df_append = df_batch.join(df_exists, on = \"id\", how = 'left_anti')\n",
    "\n",
    "        delta_table_exists = DeltaTable.forPath(self.spark, self.medallion_config.silver_path_file)\n",
    "        \n",
    "        delta_table_exists.alias(\"exist_data\")\\\n",
    "            .merge(df_append.alias(\"new_data\"), \"exist_data.id = new_data.id\")\\\n",
    "            .whenNotMatchedInsertAll()\\\n",
    "            .execute()\n",
    "    \n",
    "    def load_transform_event_data(self) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Loads and transforms event data from the Bronze layer, applying necessary transformations for the Silver layer.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame\n",
    "            The transformed Spark DataFrame.\n",
    "        Raises:\n",
    "        RuntimeError\n",
    "            If there is an error while loading or transforming the event data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = self.spark.readStream.format(self.medallion_config.bronze_format)\\\n",
    "                            .options(**self.medallion_config.bronze_opts)\\\n",
    "                            .option(\"cloudFiles.schemaLocation\", self.medallion_config.bronze_path_file)\\\n",
    "                            .load(self.medallion_config.bronze_path_file)\\\n",
    "                            .withColumn(\"time\", from_utc_timestamp(F.col(\"time\"), \"America/Bogota\"))\\\n",
    "                            .withColumn(\"date\", date_format('time', 'yyyy-MM-dd'))\\\n",
    "                            .withColumn(\"_ingested_at\",F.current_timestamp())\\\n",
    "                            .withColumn(\"_filename\",   F.input_file_name())\\\n",
    "                            .drop(\"artificial\",\"recovered\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading and transforming event data from the Bronze layer, error: {str(e)}\")\n",
    "        return df\n",
    "    \n",
    "    def write_event_data(self, df: DataFrame):\n",
    "        \"\"\"\n",
    "        Writes event data from the transformed DataFrame to the Silver layer.\n",
    "\n",
    "        Parameters:\n",
    "        df: DataFrame\n",
    "            The transformed Spark DataFrame containing event data.\n",
    "            \n",
    "        Raises:\n",
    "        RuntimeError\n",
    "            If there is an error while writing the event data to the Silver layer.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df.writeStream\\\n",
    "                .foreachBatch(self.process_batch)\\\n",
    "                .option(\"checkpointLocation\", self.medallion_config.checkpoint_silver_path)\\\n",
    "                .trigger(availableNow=True)\\\n",
    "                .start()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error writing event data to the Silver layer, error: {str(e)}\")\n",
    "    \n",
    "    def load_dimension_data(self) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Loads dimension data from the Bronze layer.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame\n",
    "            The Spark DataFrame containing dimension data.\n",
    "        Raises:\n",
    "        RuntimeError\n",
    "            If there is an error loading the dimension data from the Bronze layer.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.spark.read.parquet(self.medallion_config.bronze_path_file)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading dimension data from the Bronze layer, error: {str(e)}\")\n",
    "    \n",
    "    def write_dimension_data(self, df: DataFrame):\n",
    "        \"\"\"\n",
    "        Writes dimension data to the Silver layer, ensuring schema consistency.\n",
    "        \n",
    "        Parameters:\n",
    "        df: DataFrame\n",
    "            The transformed Spark DataFrame containing dimension data.\n",
    "        Raises:\n",
    "        RuntimeError\n",
    "            If there is an error writing the dimension data to the Silver layer.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df.distinct().write\\\n",
    "                .mode('overwrite')\\\n",
    "                .format('delta')\\\n",
    "                .option('overwriteSchema', 'true')\\\n",
    "                .save(self.medallion_config.silver_path_file)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error writing dimension data to the Silver layer, error: {str(e)}\")\n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_utils",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
